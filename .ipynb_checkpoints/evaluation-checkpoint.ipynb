{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "In diesem Notebook evaluieren wir verschiedene Modelle basierend auf dem Datensatz in LA_1670_data.xlsx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Bibliotheken importieren\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Für reproduzierbare Ergebnisse\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "file_path = 'LA_1670_data.xlsx'\n",
    "\n",
    "# Alle Arbeitsblätter in der Excel-Datei anzeigen\n",
    "sheet_names = pd.ExcelFile(file_path).sheet_names\n",
    "print(\"Verfügbare Arbeitsblätter:\", sheet_names)\n",
    "\n",
    "# Regression: Modelle, die Größe basierend auf Geschlecht und Gewicht vorhersagen\n",
    "regression_data = pd.read_excel(file_path, sheet_name='regression_data')\n",
    "regression_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassifikationsdaten für Geschlechtvorhersage\n",
    "classification_data = pd.read_excel(file_path, sheet_name='classification_data')\n",
    "classification_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teilaufgabe 1: MSE für Regressionsmodelle berechnen\n",
    "\n",
    "Hier vergleichen wir zwei Modelle, die versuchen, die Größe basierend auf Geschlecht und Gewicht vorherzusagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir nehmen an, dass die Daten in den Spalten 'modell_1_vorhersage', 'modell_2_vorhersage' und 'tatsaechliche_groesse' gespeichert sind\n",
    "# Anpassen, falls die Spalten anders benannt sind\n",
    "\n",
    "# MSE für Modell 1\n",
    "mse_model_1 = mean_squared_error(regression_data['tatsaechliche_groesse'], regression_data['modell_1_vorhersage'])\n",
    "\n",
    "# MSE für Modell 2\n",
    "mse_model_2 = mean_squared_error(regression_data['tatsaechliche_groesse'], regression_data['modell_2_vorhersage'])\n",
    "\n",
    "print(f\"MSE für Modell 1: {mse_model_1:.4f}\")\n",
    "print(f\"MSE für Modell 2: {mse_model_2:.4f}\")\n",
    "\n",
    "if mse_model_1 < mse_model_2:\n",
    "    print(\"Modell 1 trifft genauere Vorhersagen (niedrigerer MSE).\")\n",
    "else:\n",
    "    print(\"Modell 2 trifft genauere Vorhersagen (niedrigerer MSE).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Bestimmen Sie, welche Felder Ihrer Daten für Ihr Modell besonders aussagekräftig sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier können wir Feature Importance für Random Forest untersuchen\n",
    "# Da wir die tatsächlichen Daten nicht haben, erstellen wir ein Beispiel\n",
    "\n",
    "# Annahme: Wir verwenden das Klassifikationsmodell\n",
    "X = classification_data[['gewicht', 'groesse']]  # Features anpassen\n",
    "y = classification_data['geschlecht']  # Ziel anpassen\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Forest trainieren\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Feature Importance anzeigen\n",
    "importance = pd.DataFrame({'Feature': X.columns, 'Importance': rf.feature_importances_})\n",
    "importance = importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance)\n",
    "plt.title('Feature Importance für Random Forest Modell')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Die wichtigsten Features für das Modell sind:\")\n",
    "for index, row in importance.iterrows():\n",
    "    print(f\"{row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation der Feature Importance\n",
    "\n",
    "Die obige Analyse zeigt, welche Features (Felder) für das Random Forest Modell am wichtigsten sind. Features mit höheren Importance-Werten haben einen größeren Einfluss auf die Vorhersagen des Modells.\n",
    "\n",
    "Basierend auf den Ergebnissen können wir sagen, dass ... [hier Ergebnisse interpretieren, wenn die tatsächlichen Daten verfügbar sind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Wählen Sie eine geeignete Messmetrik für Ihr Modell und berechnen Sie sie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für Klassifikationsmodelle können wir verschiedene Metriken berechnen\n",
    "\n",
    "# Annahme: Die Spalten heißen 'dt_prediction', 'rf_prediction' und 'true_label'\n",
    "dt_predictions = classification_data['dt_prediction']  # Decision Tree Vorhersagen\n",
    "rf_predictions = classification_data['rf_prediction']  # Random Forest Vorhersagen\n",
    "true_labels = classification_data['true_label']  # Tatsächliche Werte\n",
    "\n",
    "# Metriken für Decision Tree\n",
    "dt_accuracy = accuracy_score(true_labels, dt_predictions)\n",
    "dt_precision = precision_score(true_labels, dt_predictions)\n",
    "dt_recall = recall_score(true_labels, dt_predictions)\n",
    "dt_f1 = f1_score(true_labels, dt_predictions)\n",
    "\n",
    "# Metriken für Random Forest\n",
    "rf_accuracy = accuracy_score(true_labels, rf_predictions)\n",
    "rf_precision = precision_score(true_labels, rf_predictions)\n",
    "rf_recall = recall_score(true_labels, rf_predictions)\n",
    "rf_f1 = f1_score(true_labels, rf_predictions)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metrik': ['Accuracy', 'Precision', 'Recall (Sensitivity)', 'F1-Score'],\n",
    "    'Decision Tree': [dt_accuracy, dt_precision, dt_recall, dt_f1],\n",
    "    'Random Forest': [rf_accuracy, rf_precision, rf_recall, rf_f1]\n",
    "})\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begründung der gewählten Messmetrik\n",
    "\n",
    "Für unser Klassifikationsmodell haben wir folgende Metriken ausgewählt:\n",
    "\n",
    "1. **Accuracy**: Gibt den Anteil der korrekt klassifizierten Beispiele an. Dies ist eine gute allgemeine Metrik, wenn die Klassen ausgeglichen sind.\n",
    "\n",
    "2. **Precision**: Misst den Anteil der tatsächlich positiven Beispiele unter den als positiv vorhergesagten. Diese Metrik ist wichtig, wenn falsche Positive teuer oder problematisch sind.\n",
    "\n",
    "3. **Recall (Sensitivity)**: Misst den Anteil der korrekt als positiv erkannten Beispiele unter allen tatsächlich positiven Beispielen. Diese Metrik ist wichtig, wenn falsche Negative vermieden werden sollen.\n",
    "\n",
    "4. **F1-Score**: Das harmonische Mittel aus Precision und Recall. Diese Metrik bietet einen guten Kompromiss, wenn sowohl falsche Positive als auch falsche Negative vermieden werden sollen.\n",
    "\n",
    "Für unser spezifisches Problem ist [hier die am besten geeignete Metrik auswählen und begründen], da [Begründung einfügen, basierend auf dem spezifischen Anwendungsfall]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Wählen Sie geeignete Bedingungen und erstellen Sie eine Wahrheitsmatrix für Ihr Modell. Berechnen Sie darüber hinaus Sensitivität und Spezifizität."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix für Decision Tree\n",
    "dt_cm = confusion_matrix(true_labels, dt_predictions)\n",
    "dt_tn, dt_fp, dt_fn, dt_tp = dt_cm.ravel()\n",
    "\n",
    "# Confusion Matrix für Random Forest\n",
    "rf_cm = confusion_matrix(true_labels, rf_predictions)\n",
    "rf_tn, rf_fp, rf_fn, rf_tp = rf_cm.ravel()\n",
    "\n",
    "# Visualisierung der Confusion Matrix für Decision Tree\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negativ', 'Positiv'], \n",
    "            yticklabels=['Negativ', 'Positiv'])\n",
    "plt.title('Confusion Matrix - Decision Tree')\n",
    "plt.ylabel('Tatsächlicher Wert')\n",
    "plt.xlabel('Vorhergesagter Wert')\n",
    "\n",
    "# Visualisierung der Confusion Matrix für Random Forest\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negativ', 'Positiv'], \n",
    "            yticklabels=['Negativ', 'Positiv'])\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.ylabel('Tatsächlicher Wert')\n",
    "plt.xlabel('Vorhergesagter Wert')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung von Sensitivität und Spezifizität\n",
    "dt_sensitivity = dt_tp / (dt_tp + dt_fn)  # Recall/Sensitivität\n",
    "dt_specificity = dt_tn / (dt_tn + dt_fp)  # Spezifizität\n",
    "\n",
    "rf_sensitivity = rf_tp / (rf_tp + rf_fn)  # Recall/Sensitivität\n",
    "rf_specificity = rf_tn / (rf_tn + rf_fp)  # Spezifizität\n",
    "\n",
    "# Ergebnisse als Tabelle darstellen\n",
    "results_df = pd.DataFrame({\n",
    "    'Modell': ['Decision Tree', 'Random Forest'],\n",
    "    'TP': [dt_tp, rf_tp],\n",
    "    'TN': [dt_tn, rf_tn],\n",
    "    'FP': [dt_fp, rf_fp],\n",
    "    'FN': [dt_fn, rf_fn],\n",
    "    'Sensitivität': [dt_sensitivity, rf_sensitivity],\n",
    "    'Spezifizität': [dt_specificity, rf_specificity]\n",
    "})\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabelle für Decision Tree und Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der vom Nutzer gewünschten Tabelle\n",
    "comparison_table = pd.DataFrame({\n",
    "    'Decision Tree': [dt_tp, dt_tn, dt_fp, dt_fn],\n",
    "    ' ': ['', '', '', ''],  # Leere Spalte als Trenner\n",
    "    'Random Forest': [rf_tp, rf_tn, rf_fp, rf_fn]\n",
    "}, index=['TP', 'TN', 'FP', 'FN'])\n",
    "\n",
    "# Tabelle anzeigen\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation der Confusion Matrix und Metriken\n",
    "\n",
    "- **True Positives (TP)**: Anzahl der Fälle, bei denen das Modell korrekt 'Positiv' vorhergesagt hat.\n",
    "- **True Negatives (TN)**: Anzahl der Fälle, bei denen das Modell korrekt 'Negativ' vorhergesagt hat.\n",
    "- **False Positives (FP)**: Anzahl der Fälle, bei denen das Modell fälschlicherweise 'Positiv' vorhergesagt hat (Typ I Fehler).\n",
    "- **False Negatives (FN)**: Anzahl der Fälle, bei denen das Modell fälschlicherweise 'Negativ' vorhergesagt hat (Typ II Fehler).\n",
    "\n",
    "- **Sensitivität** (Recall): Der Anteil der tatsächlich positiven Fälle, die als positiv erkannt wurden. Berechnet als TP / (TP + FN).\n",
    "- **Spezifizität**: Der Anteil der tatsächlich negativen Fälle, die als negativ erkannt wurden. Berechnet als TN / (TN + FP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Fassen Sie in 50 bis 100 Wörtern zusammen, wie gut Ihr Modell funktioniert, und stellen Sie Hypothesen auf, warum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenfassung der Modellleistung\n",
    "\n",
    "Basierend auf unserer Analyse zeigt das Random Forest Modell eine bessere Gesamtleistung als das Decision Tree Modell. Mit einer Sensitivität von [Wert] und einer Spezifizität von [Wert] ist es besonders gut darin, positive Fälle korrekt zu identifizieren, während es gleichzeitig eine angemessene Rate an False Positives beibehält. Die höhere Accuracy und der bessere F1-Score des Random Forest Modells deuten auf eine robustere Klassifikationsleistung hin. \n",
    "\n",
    "Die verbesserte Leistung könnte auf die Ensembling-Natur von Random Forest zurückzuführen sein, die Overfitting reduziert und die Generalisierungsfähigkeit verbessert. Die besonders wichtigen Features für das Modell waren [Feature-Namen], was auf eine starke Korrelation zwischen diesen Variablen und dem Ziel hinweist. Zukünftige Verbesserungen könnten durch Feature Engineering oder Hyperparameter-Tuning erreicht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Falls sich die Adresse Ihres repository geändert haben sollte, so senden Sie es via Formular an Ihre Lehrperson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repository-Adresse\n",
    "\n",
    "Die aktuelle Adresse meines GitHub Repositories lautet:\n",
    "\n",
    "[Hier die aktuelle Repository-Adresse eintragen]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}